Based on the iterative discussion in the sources, here is the consolidated **Implementation Specification** for your Duplicate File Detection System.

This summary aggregates the final "frozen" decisions, ignoring earlier ideas that were refactored (e.g., switching from SQLite to MongoDB, and from `LocalImageScanner` to `LocalFileScanner`).

### 1. Core Architectural Standards
*   **Identity Principle:** File identity is defined strictly by **Content Hash** (MD5/SHA-256). Metadata (timestamps, filenames) is used only for description or caching, never for identity.
*   **Drive Strategy (No-Download):** You do not download Drive files. You fetch Drive metadata (specifically `md5Checksum`) and compare it against local hashes.
*   **Concurrency:** Parallelize per-file (not per-chunk). Limit to **~4 workers** to prevent disk thrashing.

### 2. Tech Stack & Configuration
*   **Language:** Python.
*   **Persistence:** **MongoDB** (chosen over SQLite for scale and schema flexibility).
*   **Hashing Algorithm:** Streaming SHA-256 (Canonical) or MD5 (Fast/Drive-compatible). Use 1MB chunks.

### 3. The "Frozen" Folder Structure
This is the final agreed-upon module structure using Clean Architecture/SOLID principles:

```text
GFM/
│
├── app/
│   ├── __init__.py
│   ├── main.py                  # ENTRY POINT (Wiring, Config, CLI invoke)
│   └── config.py
│
├── domain/                      # PURE BUSINESS LOGIC
│   ├── __init__.py
│   ├── models.py                # FileRecord entity
│   ├── interfaces.py            # Abstract Base Classes (Ports)
│   ├── rules.py                 # Confidence scoring logic
│   └── exceptions.py
│
├── services/                    # USE CASES
│   ├── __init__.py
│   ├── inventory_runner.py      # ORCHESTRATOR (Coordinates scan->hash->save)
│   ├── hashing_service.py       # Streaming logic
│   ├── comparison_service.py    # Logic to match Local vs Drive
│   └── deletion_service.py      # Safety checks and execution
│
├── infrastructure/              # ADAPTERS
│   ├── local/
│   │   ├── local_scanner.py     # Generic file discovery
│   │   └── file_reader.py
│   ├── drive/
│   │   ├── drive_client.py
│   │   └── drive_scanner.py     # Metadata fetcher
│   └── persistence/
│       └── mongo_hash_repo.py   # MongoDB implementation
│
└── utils/                       # SHARED
    ├── logging.py
    └── validators.py
```

### 4. Data Models (Domain Layer)
**Entity:** `FileRecord`
This schema was "frozen" to ensure stability.
```python
@dataclass(frozen=True)
class FileRecord:
    path: str
    name: str
    extension: str
    size: int
    last_modified: float
    hash: Optional[str]
    hash_algo: str
    source: str = "local"  # or "drive"
```

### 5. Component Implementations
The discussion produced specific code patterns for the core components:

#### A. Generic Local Scanner (`infrastructure/local/local_scanner.py`)
Refactored from an image-only scanner to a generic file scanner.
*   **Responsibility:** Discover files and collect raw metadata. No business rules or filtering.
*   **Logic:** Recursively walks directories; captures path, size, and mtime.

#### B. Streaming Hashing (`services/hashing_service.py`)
Implemented to handle large files (GBs) without memory spikes.
*   **Logic:** Reads file in `1024 * 1024` (1MB) chunks.
*   **Method:** `stream_hash(path, algo)` using `hashlib.update()`.

#### C. MongoDB Repository (`infrastructure/persistence/mongo_hash_repo.py`)
Handles caching to prevent re-hashing unchanged files.
*   **Schema:** Append-only.
*   **Indexes:**
    *   Compound Unique Index: `{ path: 1, last_modified: 1 }` (To identify unchanged files).
    *   Index: `{ hash: 1 }` (For fast duplicate lookups).
*   **Logic:** `upsert` records based on path + modified time.

#### D. Orchestration (`services/inventory_runner.py`)
Clarified as the use-case runner, distinct from `main.py`.
*   **Flow:** Call Scanner -> Check Repo (Cache Hit?) -> If Miss, Call Hasher -> Save to Repo.

### 6. Safety & Deletion Logic
#### Confidence Score System
Before deleting anything from Drive, the system calculates a score:
*   **MD5 Match:** 70 points.
*   **Size Match:** 20 points.
*   **Extension Match:** 5 points.
*   **Filename Match:** 5 points.
*   **Threshold:** Delete only if **Score ≥ 90** (Requires Hash + Size at minimum).

#### Execution Safeguards
*   **Dry Run:** Mandatory first pass. Outputs a report (CSV/Excel) listing `Confidence Score` and `Proposed Action` (KEEP/DELETE).
*   **Action:** Files are moved to **Trash**, not permanently deleted.

